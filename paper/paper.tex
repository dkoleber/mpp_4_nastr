\documentclass[twocolumn]{article}
\usepackage[bottom=10em,top=10em,right=5em,left=5em]{geometry}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[numbers]{natbib}
\usepackage{pgfplots}
\usepackage{layouts}
\usepackage{placeins}
\usepackage{layout}

\bibliographystyle{plainnat}
\setlength{\headsep}{5pt}


\title{Placeholder Title}

\author{%
  Derek Koleber \\
  \texttt{derekkoleber@gmail.com}  
}

\begin{document}

\maketitle

\begin{abstract}
    One of the saught after objectives within neural architecture search
    is the reduction of search time. Some recent approaches attempt to reduce
    search time by using surrogate performance metric that predict performance
    based on learning curves and/or architecture embeddings. 
    In this paper, the efficacy of purely statistical methodology for accelerating
    the search time of evolutionary algorithm oriented NAS approaches is assessed. Analysis of results
    suggests that statistical methods do not provide meaningful search time reductions
    within commonly used NAS search spaces.
\end{abstract}

\section{Introduction}

    In recent years, various approaches to neural architecture search (NAS) have been 
successful in discovering neural architectures in a variety of computer vision
tasks as well as in language modeling, and INSERT SOMETHING. While many discovered architectures
have achieved state-of-the-art performance on popular computer benchmarks,
the compute cost of NAS acts as a significant obstacle.

    Works within the field have taken many different approaches to reducing search time.
Of the most significance to this work is the approach of using a performance surrogate,
which allows the prediction of an architecture's performance prior to training completion.
The effectiveness of different executions of approach manifest themselves in efficiency improvements 
all the way from early stopping to \cite{arch_pred_1} architecture analysis without the necessity to train.
\cite{snas} \cite{darts} 
\cite{arch_pred_1}
\cite{econas}
\cite{amoebanet}
\cite{acc_pred_1}
\cite{pnas}
\cite{lemonade}
\cite{nasnet}
\cite{learning_curve_1}
\cite{synthetic_samples}
\cite{snas}
\cite{enas}
\cite{cars}
\cite{rl_nas}
\cite{evolved_transformer}
\cite{multitask_1}
\cite{mnas}
\cite{obj_detection_1}
\cite{hardware_darts}



\subsection{Background and Related Work}




For NAS approaches utilizing genetic evolutionary algorithms, the means with which the algorithm determines the most and least fit candidates 
for mutation and optionally removal hinges upon the ability to determine comparative performance between candidates. The go-to metric for 
determining the performance of candidates with single-objective models is validation or test accuracy or error. However, recent approaches 
have forgone ratio variables to measure perforance in favor of ordinal variables \cite{pnas}, since relative performance is the only necessity 
for common selection strategies like traditional tournament selection and its variations, such as aging selection \cite{amoebanet}. 
In the case of PNAS \cite{pnas}, a surrogate model is used to predict model performance in a way that ranks models approximately as they would
be ranked as they would be using actual performance.


\subsection{Approach}

Using a surrogate model to predict model rank has the downside of training surrogate model to predict in a space that's growing in parallel with the
surrogate. PNAS \cite{pnas} proposes a clever solution to this which allows the surrogate to train over more data during the beginning of search.
Its proposed approach is only possible due to the fact that the PNAS search space has the capability to expand its search space in a way that
allows the initial, smaller search spaces to be representative of the later, larger search spaces in a way that is meaningful to the surrogate.
Such an approach doesn't provide benefits to search spaces that aren't expandable in a manner similar to that of PNAS, which means a surrogate would
have to train on architectures representing a complete search space. 


Surrogate models are not the only way to predict candidate ranking. Two ways to represent relative performances of a population of candidates are the following 'performance metrics':

- Raw Rank Measurements Based on Candidate Accuracy

Rank measurements are a clear way to represent accuracies as ordinal data. No additional explanation is necessary.

- Z-Scores of Candidate Accuracy

Zz-scores of candidate performance can somewhat act as interval data and thus can better represent outliers and clusters in comparison to raw ranks. 
Z-Scores are typically used with normal distributions.
Model accuracies in NASBench 201 \cite{nasbench201} provide a complete view of the distributions of accuracies in what could be considered to be 
a search space representative of common NAS search spaces. 
An Anderson-Darling test over model test accuracies for Cifar10 (200 epochs) in NASBench201 produces a test statistic of 2717.199 
(718.514 after a Box-Cox transform) indicates at all significance levels that model accuracies do not fall into a normal distribution. 
Nonetheless, z-scores still provide a way to judge relative model performance.

The relevant benefit of ordinal and interval performance metrics is that a population's measurements evaluated at an early epoch are 
directly comparable to its measurements at a later epoch. This is the basis for the possibility of search time acceleration, since
the early performances have the potential to be predictive of final performances.

% \printinunitsof{in}\prntlen{\textwidth}

\subsection{Stochastic Search Experimentation}

\subsubsection{Experiment Details}
To measure the potential of the use of z-scores and raw ranks as a predictors of final performance, populations of models are used to
emulate a stochastic search that is accelerated by early performance prediction. 
While stochastic search is a far-from-optimal search algorithm, this experiment's goal is rather to provide insight into the reliability 
using the two performance predictors to find the best candidates at different population sizes. 

Two populations of trained models from two different architecutre search spaces are sampled for this purpose.
The first population is generated using a variety of hyperparemeter combinations within a search space similar to that of NASNet \cite{nasnet}.
Implementation details of this search space are provided in the appendix. 
The second population is a sampled subset of the architectures in NASBench201 \cite{nasbench201}.

Each population $P$ is evaluated at each combination between elements of $E^s$, $W^s$, and $M$.
Each such evaluation at a given combination is referred to as a \emph{simulation}.
The set of \emph{prediction epoch scalars} $E^s$ scales $t$, the total number of epochs per model in $P$,
to produce the actual \emph{prediction epochs} at which final model performance rank will be predicted, $E^{a}$.
This facilitates early performance prediction and thus the potential for search acceleration.
The set of \emph{prediction window scalars} $W^s$ scales $E^a$ to produce the actual \emph{prediction windows} to be used 
over trained epochs to be used during evaluation, $W^a$.
The set of \emph{evaluation metric} functions $M$ includes functions for calculating an evaluation metric,
given a model, a prediction epoch, and a prediction window. 
These metric functions include averaged-over-window z-score measurements \emph{zm}'s, and raw rank measurements \emph{rm}'s.

\[E^s = \{1, 0.5, 0.25, 0.125\}\]
\[W^s = \{1, 0.5, 0.25, 0.001\}\]
\[M = \{f_{zm}, f_{rm}\}\]

\[f_{zm}(x, e, w) = \frac{1}{e-w}\sum_{i=e-w}^{e}\frac{x_i - \mu_i}{\sigma_i}\]
\[f_{rm}(x, e, w) = \frac{1}{e-w}\sum_{i=e-w}^{e}r_{x_{i}}(P)\]

\[predictions(P, e, w, f_{predictor}) = r(map(P, f_{predictor}))\]

\[r(P) = rank\ of\ every\ element\ in\ P\]
\[r_{x}(P) = rank\ of\ x\ within\ P\]

In each simulation, $P$ is shuffled then evaluated as an incremental subset of the shuffled population.
Thus, each simulation evaluates a simulated population as if it were growing to its final size, while using
the evaluation metrics to estimate simulated population rankings at each step.
This allows a direct comparison between the predicted rankings and actual rankings at each increment.
Simlar to PNAS \cite{pnas}, the Spearman Coefficient is chosen to judge the reliability of early prediction of population performance ranks.
At the addition of each subsequent model to the simulated population, average rank error, rank error for the newest model (new rank error), and the Spearman Coefficient
for the population are measured and used as evaluation metrics. 
Each simulation is run $N$ times, and evaluation metrics are averaged across simulation iterations. 
Refer to the Figure ~\ref{fig:nasnet_1x_acceleration}, ~\ref{fig:nasnet_2x_acceleration}, ~\ref{fig:nasnet_4x_acceleration}, and ~\ref{fig:nasnet_8x_acceleration} in Appendix for simulation results.
Additionally, the correlation between new models' predicted rank error and new models' predicted rank is measured for each simulation,
and results are averaged across simulation iterations. These results can be found in Figure ~\ref{fig:rank_error_correlations}.

\subsubsection{Analysis}

\textbf{Higher acceleration rates result in higher average and new rank error, and lower Spearman coefficients.}
New rank error and Spearman ccoefficients converge to levels that show clear trend that increases with acceleration rate.
This is unsurprising, as these measurements are based on model performance that are further away in terms of epochs to their
otherwise final performance. 
As acceleration rate increases, average rank error comes closer to stabilizing around $\frac{1}{3}$, the expected average error rate
for a random population, but the rank predictions causing such a rank error are clearly not random since the Spearman coefficient is far from zero.

\textbf{New rank error stabilizes at a lower value than average rank error.}
In all simulations, new rank error appears to stabilize at approximately half the value of the average rank error.
Additionally, new rank error stabilizes faster than average rank error, stabilizing near a population of 4 while
average rank error begins to stabilize around a population of 12.

\textbf{Model size appears to impact performance prediction accuracy.} 
The NASNet model population is comprised of models which all are larger than their NASBench201 counterparts.
With NASNet simulations, Spearman coefficients remain consistenly higher and new rank error levels off at a lower value in comparison to NASBench201 simulations.
Nonetheless, NASNet simulations show an average rank error trajectories similar to the trajectories that NASBench201 simulations play out.
\textbf{Where do average rank error/new rank error/spearman coeff actually level out to? - take last quarter and average it?}

\textbf{Prediction based on zscores and prediction based on raw ranks produce virtually the same results.}
This is more evident with NASBench201 since it draws out to a higher population size. 
\textbf{CORRELATION?}

\textbf{New rank error has an overall weak-to-moderate positive correlation with the predicted new rank itself.}
This trend is better observed with NASBench201 simulations, due to the fact that its larger simulated population will produce an innately more accurate
representation of such a trend. 
\textbf{HOW DOES THIS CORRELATE WITH INCREASING ACCELERATION RATE?}
This trend can be interpreted optimistically to suggest that, in an evolution context, degrees of exploration will be bolstered
outside of the exploration facilitated by the evolutionary algorithm alone. 
Pessimistically, this trend can be interpreted to suggest that suboptimal candidates might be chosen during the selection step of
evolutionary selection algorithms, resulting in slower time to find better candidates or the possibility for better candidates
to be spuriously removed.


\section{Aging Search Experimentation}

Results from stochastic search experimentation are only applicable to an evolutionary algorithm context during the population's development prior to 
selection mechanisms taking effect, since selection mechanisms generally increase the frequency of higher-performance candidates. 
With a higher concentration of highly-performaning candidates in a population, the given evaluation metrics might perform less reliably.



testtest







\section{Headings: first level}
\label{headings}

All headings should be lower case (except for first word and proper nouns),
flush left, and bold.

First-level headings should be in 12-point type.

\subsection{Headings: second level}

Second-level headings should be in 10-point type.

\subsubsection{Headings: third level}

Third-level headings should be in 10-point type.

\subsection{Footnotes}

Footnotes should be used sparingly.  If you do require a footnote, indicate
footnotes with a number\footnote{Sample of the first footnote.} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches (12~picas).

Note that footnotes are properly typeset \emph{after} punctuation
marks.\footnote{As in this example.}

\subsection{Figures}

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
\end{figure}

All artwork must be neat, clean, and legible. Lines should be dark enough for
purposes of reproduction. The figure number and caption always appear after the
figure. Place one line space before the figure caption and one line space after
the figure. The figure caption should be lower case (except for first word and
proper nouns); figures are numbered consecutively.

You may use color figures.  However, it is best for the figure captions and the
paper body to be legible if the paper is printed in either black/white or in
color.

\subsection{Tables}



\begin{table}
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}

\bibliography{reference}

\FloatBarrier
\clearpage
\onecolumn
\appendix
\section{Appendix}
\subsection{Stochastic Search Results}
\subsubsection{NASNet Simulations}
\begin{figure}[!h]
    \begin{center}
        \input{../evolution/nasnet_1.0x_acceleration.pgf}
    \end{center}
    \caption{NASNet Simulation with 1x Acceleration}
    \label{fig:nasnet_1x_acceleration}
\end{figure}
\begin{figure}
    \begin{center}
        \input{../evolution/nasnet_2.0x_acceleration.pgf}
    \end{center}
    \caption{NASNet Simulation with 2x Acceleration}
    \label{fig:nasnet_2x_acceleration}
\end{figure}

\begin{figure}
    \begin{center}
        \input{../evolution/nasnet_4.0x_acceleration.pgf}
    \end{center}
    \caption{NASNet Simulation with 4x Acceleration}
    \label{fig:nasnet_4x_acceleration}
\end{figure}

\begin{figure}
    \begin{center}
        \input{../evolution/nasnet_8.0x_acceleration.pgf}
    \end{center}
    \caption{NASNet Simulation with 8x Acceleration}
    \label{fig:nasnet_8x_acceleration}
\end{figure}

\begin{figure}
    \begin{center}
        \input{../evolution/nasnet_rank_error_correlations.pgf}
    \end{center}
    \caption{NASNet Rank Error Correlations with Rank}
    \label{fig:nasnet_rank_error_correlations}
\end{figure}

\FloatBarrier
\clearpage
\subsubsection{NASBench201 Simulations}

\begin{figure}[!h]
    \begin{center}
        \input{../evolution/nasbench_16_1.0x_acceleration.pgf}
    \end{center}
    \caption{NASBench201 Simulation with 1x Acceleration}
    \label{fig:nasbench_1x_acceleration}
\end{figure}

\begin{figure}
    \begin{center}
        \input{../evolution/nasbench_16_2.0x_acceleration.pgf}
    \end{center}
    \caption{NASBench201 Simulation with 2x Acceleration}
    \label{fig:nasbench_2x_acceleration}
\end{figure}

\begin{figure}
    \begin{center}
        \input{../evolution/nasbench_16_4.0x_acceleration.pgf}
    \end{center}
    \caption{NASBench201 Simulation with 4x Acceleration}
    \label{fig:nasbench_4x_acceleration}
\end{figure}

\begin{figure}
    \begin{center}
        \input{../evolution/nasbench_16_8.0x_acceleration.pgf}
    \end{center}
    \caption{NASBench201 Simulation with 8x Acceleration}
    \label{fig:nasbench_8x_acceleration}
\end{figure}

\begin{figure}
    \begin{center}
        \input{../evolution/nasbench_16_rank_error_correlations.pgf}
    \end{center}
    \caption{NASBench201 Rank Error Correlations with Rank}
    \label{fig:nasbench_rank_error_correlations}
\end{figure}

\FloatBarrier
\clearpage
\subsection{NASNet Population Details}

Using these possible ops

Each evaluated to 16 epochs, how big is population?

hyperparameters are as follows.

UPDATE CHARTS TO INDICATE WHAT ORANGE/BLUE MEANS

\end{document}
