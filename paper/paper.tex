\documentclass[twocolumn]{article}
\usepackage[bottom=10em,top=10em,right=5em,left=5em]{geometry}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[numbers]{natbib}
\usepackage{pgfplots}
\usepackage{layouts}
\usepackage{placeins}
\usepackage{layout}
\usepackage{booktabs}       % For \toprule, \midrule and \bottomrule
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{multirow}
\usepackage[font=small,labelfont=bf]{caption}

\usepackage{pgfplotstable}  % Generates table from .csv
\usepackage{siunitx}        % Formats the units and values
\sisetup{
    round-mode=places,
    round-precision=3,
    scientific-notation=false,
}
\usepgfplotslibrary{external} 

\tikzexternalize

\bibliographystyle{unsrtnat} %{plainnat}
\setlength{\headsep}{5pt}

\title{Model-less Performance Prediction for Neural Architecture Search Time Reduction}
\date{First released September 8, 2020, Last edited \today}

\author{%
  Derek Koleber \\
  \texttt{derekkoleber@gmail.com}  
}

\begin{document}

\maketitle

\begin{abstract}
    One of the sought after objectives within neural architecture search
    is the reduction of search time. Many recent approaches attempt to reduce
    search time by using surrogate models that predict performance
    based on learning curves or model topologies. 
    In this paper, the efficacy of a statistical methodology for reducing
    the search time of an evolutionary-algorithm-oriented architecture search is assessed. 
    This methodology uses two different windowed measures of relative architecture preformance
    to predict final relative performance.
    Results suggest that such methodology does not enable search time reduction via early stopping, 
    but does improve search time by using a more efficent performance metric than a direct measure of model performance.
\end{abstract}

\section{Introduction}

In recent years, various approaches to neural architecture search (NAS) have been 
successful in discovering high-performance neural network architectures in a variety of spaces, primarily computer vision.
Many discovered architectures have achieved state-of-the-art performance on popular benchmarks such as Cifar10 and ImageNet \cite{nasnet}\cite{amoebanet}\cite{enas}\cite{darts}\cite{econas},
but this is not without compute cost acting as a significant obstacle.

Works within the field have taken on the reduction search time from multiple directions.
One common approach is the use of a trained surrogate predictor model \cite{bayesian_pred}\cite{vsvr_pred}\cite{peephole}\cite{pnas},
which allows the prediction of an architecture's performance prior to training completion, or sometimes prior to training altogether.
This, however, comes with the intrinsic downside of an initially untrained predictor model requiring training data from the search space prior to being able to 
act in the search space, creating a circular dependency that guarantees initially inefficiency during the search process.
There are solutions to getting around this dilemma \cite{pnas}, but early stopping facilitated by a model-less performance predictor
avoids this problem entirely, since training the predictor is not required.

This work explores the utilization of a population of candidates' relative performances, 
measured as direct rankings or z-scores then averaged across a window of trained epochs,
as a means to predict candidate rankings before training completion. 
Results indicate this method, in combination with early stopping, overall does not result in higher performing candidates being found sooner.
However, the use of a window over the entirety of trained epochs is shown to result in 
the discovery of better candidates in comparison to a baseline search that only utilizes the candidates' final performances, 
and does so at a faster rate compared to the baseline.

This method stands to improve search times and search results for approaches with candidates trained and incrementally evaluated across a number of epochs,
meaning this work has a high level of potential applicability across the neural architecture search field.

\subsection {Neural Architecture Search}

The field of neural architecture search was pioneered in \emph{Neural Architecture Search with Reinforcement Learning} \cite{rl_nas}, which
positioned the field as, ``a new research direction for automatically finding good neural network architectures.''
This work performed a single-objective search for an image classifier architecture with an unbounded search space, achieving SOTA on Cifar-10.
Since then, the field has progressed significantly, all the while diversifying the approaches and goals for neural architecture search.

NASNet \cite{nasnet} pushed the field forward in the direction of scalability and transferrability.
NASNet used a bounded search space in the form of cells, which was combined with a fixed macro-architecture for different tasks.
Cells that performed well on Cifar-10 within a smaller macro-architecture were shown to also perform well on ImageNet within a larger macro-architecture,
showing the viability of a proxy objective in combination with a cell-based architecture search.

Some works have innovated in the direction of the search mechanism, moving away from a reinforcement learning driver, toward evolution-based search\cite{amoebanet}
or differentiable architecture search \cite{darts}.

Other works have investigated alternative search objectives, such as implementing platform-aware searches \cite{mnas} \cite{hardware_darts}, 
or multi-objective searches \cite{lemonade} which explore Pareto fronts across network performance and network size.

While the majority of recent works have remained within the image classification space, some have broken out of computer vision
altogether to instead address spaces such as language modelling, as is the case with \emph{The Evolved Transformer} \cite{evolved_transformer}.

\section{Related Work}

The goal of the reduction of search time has been achieved through approaches such as parameter sharing \cite{enas} \cite{cars},
the use of smaller performance proxies \cite{econas}, and surrogate performance preditors.

PNAS\cite{pnas} used a reinforcement-learning-based surrogate model which predicted relative network performance based on cell topology to drastically
reduce search times for a cell-based architecture search. 
Similarly, \emph{Peephole} \cite{peephole} predicts network performance based on network topology using an LSTM agent.
A notable shortcoming of these approaches is the fact that the surrogate model requires retraining between fundamentally different cell topologies.

Rather than relying on cell topology to predict performance prior to training, some works attempt to predict performance based on learning curves\cite{bayesian_pred}\cite{vsvr_pred}.
This was initially done with the objective of hyperparameter optimization with a weighted probabilistic learning curve model\cite{hparam_opt_1}.
This was built upon with the addition of Bayesian models \cite{bayesian_pred} and \emph{v}-SVRs \cite{vsvr_pred}.
Common among these approaches that utilize the learning curve for performance prediction is the prediction of model performance directly,
rather than the prediction of a metric that correlates with model performance.

Direct prediction of network performance is not always necessary. 
As discussed in PNAS \cite{pnas}, the important quality of a predictor is the ability to rank models in same order as they would be ranked if ordered
according to their actual performances.
This allows for the selection of the best models irrespective of their actual performances.

A shortcoming of some surrogate model approaches \cite{peephole}\cite{bayesian_pred}\cite{vsvr_pred} is the fact that the surrogate model must be trained prior to or during the architecture search.
PNAS \cite{pnas} proposes a solution to this which allows the surrogate to train quickly at the beginning of the search, while scaling as the search progresses.
More specifically, the PNAS search space has the capability to expand its search space such that the initial, smaller search space is representative of the later,
larger search space in a way that maintains the relevance of the surrogate.

\section{Methodology}

All trained surrogate models come with the downside of requiring some amount of training data prior to being able to effectively predict performance.
In this section, intrinsic properties of a population are utilized to predict relative performances.

Two ways to represent relative performances of a population of candidates are the following \emph{performance metrics}:
\begin{itemize}
    \item \emph{Raw ranks}: Given candidates trained to the same epoch, 
        candidates can be evaluated at epochs up to, and including, the final epoch to determine their relative rankings based on performances at those epochs.
    \item \emph{Z-scores}: Similar to raw ranks, candidates can be evaluated at an epoch according to the z-scores of their performances at those epochs.
        Z-scores have the added benefit over raw ranks of providing a better indication of outliers in clusters, as is inherent with interval data 
        in comparison to ordinal data.
        Some of the meaningfulness of z-scores is lost due to the fact that model performances do not fall in a normal distribution for the most prominent 
        search space used later in this work, NASBench 201 \cite{nasbench201}.
        In this space, the final performances of models trained to 200 epochs on Cifar10 result in an Anderson-Darling test statistic of 2717.199 without a 
        Box-Cox transform, and 718.514 with, strongly indicating that model performances do not fall within a normal distribution.
        Nonetheless, z-scores still provide more information about relative model performances than raw ranks.

\end{itemize}

To be explicit, the relevant benefit of ordinal and interval performance metrics is that a population's measurements evaluated at an early epoch are 
directly comparable to its measurements at a later epoch. 
This is the basis for the possibility of search time reduction via early stopping, since the early performances have the potential to be predictive of final performances.

Specifically, search time is reduced by early stopping at epochs specified by scaling the total number of epochs that would otherwise be trained $T$ by 
\emph{prediction epoch scalars} in $E^s$,

\[E^s = \{1, 0.5, 0.25, 0.125\}\]

Scaling $T$ by $E^s$ produces the actual \emph{prediction epochs} $E^{a}$ at which final model performance rank will be predicted.
A number of epochs up to the prediction epoch are used to predict final performance. 
The number of epochs which are used are specified by a \emph{prediction window} which is determined by scaling the prediction epoch
by a \emph{prediction window scalar} from $W^s$

\[W^s = \{1, 0.5, 0.25, 0.001\}\]

This determines actual prediction windows $W^a$.
Within each window, performance metrics defined by $M$ can be used to calculate relative performances at each epoch within the window.

\[M = \{f_{zm}, f_{rm}\}\]

As discussed before, these performance metrics include z-score measurements \emph{ZM}'s, and raw rank measurements \emph{RM}'s.
Performance metrics are averaged over the window to produce final performance predictions for a population $P$.

\[f_{zm}(x, e, w) = \frac{1}{e-w}\sum_{i=e-w}^{e}\frac{x_i - \mu_i}{\sigma_i}\]
\[f_{rm}(x, e, w) = \frac{1}{e-w}\sum_{i=e-w}^{e}r_{x_{i}}(P)\]
\[predictions(P, e, w, f_{predictor}) = r(map(P, f_{predictor}))\]
\begin{center}
where
\end{center}
\[r(P) = rank\ of\ every\ element\ in\ P\]
\[r_{x}(P) = rank\ of\ x\ within\ P\]

\section{Experiments}

\subsection{Stochastic Search Experiments}
To measure the effectiveness of the use of z-scores and raw ranks as a predictors of final performance, populations of models are used to
emulate a stochastic search that is accelerated by early performance prediction. 
Stochastic search provides the same expected outcome as other uninformed search strategies like a sequential, brute force search.
This is contrary to three of the popular categories of search strategies in NAS, such as gradient descent strategies represented by DARTS\cite{darts}, 
evolutionary algorithm strategies represented by PNAS\cite{pnas}, or reinforcement learning strategies represented by ENAS\cite{enas}, all of which are informed searches.
That being said, the uninformed nature of stochastic search is made irrelevant by the experiment's goal to provide insight into the reliability 
of the two performance predictors at finding the highest performing candidates at different population sizes. 

\subsubsection{Experiment Details}
Three populations of trained models from two different architecutre search spaces are sampled.
The first two populations are sampled subsets of the architecture space available in NASBench 201 \cite{nasbench201}.
Each of these populations contains four subpopulations which are randomly sampled from the space.
The first population, \emph{NASBench 201 100p}, has subpopulations consisting of 100 models each, the size of which is chosen to mirror the working population size 
used in aging tournament selection experiments later in this work.
The second population, \emph{NASBench 201 16p}, has subpopulations consisting of 16 models each, the size of which is chosen to mirror the number of models in the 
subpopulations of the third population.
The third population, \emph{NASNet 16p}, is generated by six different hyperparameter configurations within a search space similar to that of NASNet \cite{nasnet}.
Though the population will be referred to in this work as NASNet 16p, this search space has different available operations compared to NASNet \cite{nasnet}, 
with specifics and implementation details present in the appendix.
The subpopulations for each hyperparameter configuration consists of 16 models each.
The purpose of this population is to be comparable in population size to the second population, but with significantly more network parameters.
Specifically, the smallest average parameter count among all NASNet 16p subpopulations is 1766642.5, while the estimated average for NASBench 201 is 99638.5, assuming
the average parmeter count exposed through the NASBench 201 API, which is measured in MB, represents float32 parameters.

Each subpopulation $P$ undergoes a grid search, evaluating each combination between elements of $E^s$, $W^s$, and $M$.
Each such evaluation at a given combination is referred to as a \emph{simulation}.
In each simulation, $P$ is shuffled, then evaluated incrementally.
Thus, each simulation evaluates a simulated population as if it were growing to its final size, while using
the prediction metrics to estimate simulated population rankings at each step.
This allows a direct comparison between the predicted rankings and actual rankings at each increment.
Simlar to PNAS \cite{pnas}, a Spearman coefficient is chosen to judge the reliability of early prediction of population performance ranks.
At the addition of each subsequent model to the simulated population, proportaional average rank error (\emph{PARE}), proportional rank error for the newest model (\emph{PNRE}), and the Spearman coefficient
for the simulated population are measured and used as \emph{evaluation metrics}.
Each simulation is run $N$ times, and evaluation metrics are averaged across simulation iterations. 

Each configuration within the grid search is averaged over the last 25\% of the simulation to produce approximate points of convergence for each evaluation metric 
(Table~\ref{table:nasnet_convergence},~\ref{table:nasbench_16_convergence},~\ref{table:nasbench_100_convergence}).
Additionally, the correlation between new models' predicted ranks and the corresponding PNREs is measured for each simulation,
and results are averaged across simulation iterations, subpopulations, prediction windows scalars, and prediction metrics to produce a direct mapping from prediction epoch scalar to correlation (Table~\ref{table:rank_error_corr}).
Individual simulation results are availble in the Stochastic Search Results section in the appendix.

\begin{table}[!h]
    \begin{center}
        \begin{tabular}{c | c | c | c | c}
            \toprule
            & \multicolumn{4}{c}{Prediction Epoch Scalar} \\
            Population          & 1    & 0.5    & 0.25  & 0.125 \\
            \midrule
            NASBench201 16p     & 0.194 & 0.275 & 0.342 & 0.350 \\
            NASBench201 100p    & 0.237 & 0.423 & 0.430 & 0.459 \\
            NASNet 16p          & 0.217 & 0.402 & 0.389 & 0.345 \\
            \bottomrule
        \end{tabular}
        \caption{Stochastic Simulation: PNRE Correlation with Rank}
        \label{table:rank_error_corr}
    \end{center}
\end{table}

\subsubsection{Analysis}

Unless stated otherwise, NASBench201 100p will be assumed to be the subject of analysis, since its increased population size decreases potential variance in measured properties.

\textbf{Earlier stopping results in higher new rank error, and lower Spearman coefficients.}
New rank error and Spearman coefficients converge to levels that show clear trends which decrease and increase respectively with the prediction epoch scalar.
This is unsurprising, as these measurements are based on model performances that are further away in terms of epochs to their
otherwise final performance. 
Independent of early stopping, average rank error appears to converge to $\frac{1}{3}$.
This value is signicant since it's the expected distance between two points randomly sampled from two uniformly distributed variables over $[0, 1]$.
The predicted and actual rankings, when divided by the simulated population size, are two such uniformly distributed variables, 
thus it would also be expected for rankings that are predicted randomly to have a PARE near $\frac{1}{3}$.
However, the rank predictions causing such a PARE are clearly not random since the Spearman coefficient is far from zero.

\textbf{New rank error stabilizes at a lower value than average rank error.}
In all simulations, new rank error appears to stabilize at approximately half the value of the average rank error.
Additionally, new rank error stabilizes faster than average rank error, stabilizing near a population of 4 while
average rank error begins to stabilize around a population of 12.
This suggests that new additions to the populations are more reliably ranked than existing candidates,
but this is likely driven by outliers within the existing populations resulting in an inflated average.

\textbf{Prediction based on zscores and prediction based on raw ranks produce virtually the same results.}
In Table~\ref{table:nasbench_100_convergence}, there are minimal cases where differences between ZM and RM values for PARE, PNRE, or Spearman coefficients are greater than 5\%,
and cases where there is greater than a 5\% discrepancy form no clear trend.
In Tables~\ref{table:nasnet_convergence} and~\ref{table:nasbench_16_convergence}, there are more such cases, but there still does not appear to be a trend.
In fact, ZM and RM are definitionally the same for populations trained to a number of epochs $X$ when

\[1 >= XE^{s}W^{s} = W^{a}\]

\textbf{PNRE has an overall weak-to-moderate positive correlation with the predicted new rank.}
Averaged across all prediction epoch scalars providing early stopping, the correlation between PNRE and predicted new rank is $0.378$,
indicating a weak-to-moderate positive correlation. 
This can be interpreted optimistically to suggest that, in an evolution context, the degree of exploration will be bolstered
outside of the exploration facilitated by the evolutionary algorithm alone. 
Pessimistically, this can be interpreted to suggest that suboptimal candidates might be chosen during the selection step of
evolutionary selection algorithms, resulting in slower time to find better candidates or the possibility for better candidates
to be spuriously removed from the population.

\textbf{Model size appears to impact performance prediction accuracy.} 
As mentioned before, NASNet 16p is comprised of models which all are larger than their NASBench 201 16p counterparts.
More importantly, the search space for the NASNet search space variant used in this work is $3.299\cdot10^{11}$ times larger than the NASBench 201 search space.
With NASNet 16p simulations, Spearman coefficients remain consistenly higher and PNRE levels off at a lower value in comparison to NASBench 201 16p simulations.
Nonetheless, NASNet 16p simulations show PARE trajectories similar to the trajectories of NASBench 201 16p simulations.
Overall, this suggests that performance prediction is more effective on larger search spaces in comparison to smaller ones.

\subsection{Evolutionary Search Experiments}

Results from stochastic search experimentation are only applicable to an evolutionary algorithm context during the population's development prior to 
selection mechanisms taking effect, since selection mechanisms generally increase the frequency of higher-performance candidates. 
With a higher concentration of highly-performing candidates in a population, the given evaluation metrics might perform less reliably.
To determine the reliability of the evaluation metrics in the context of an evolving population, NASBench 201 is used as a search space
over which aging tournament selection \cite{amoebanet} is used to conduct a search with a fixed time budget.

\subsubsection{Experiment Details}

This experiment will use the same population configuration and evolutionary algorithm used in Amoebanet \cite{amoebanet}.
To start the search, an initial population of 100 is established.
Then, at each selection step, 25 candidates are randomly selected; the best of which produces a mutated offspring.
After the addition of the new, mutated offspring, the oldest candidate in the population is removed to maintain a population of 100.

Similar to the previous experiment, a grid search is conducted over $E^s$, $W^s$, and $M$.
Each combination is simulated 64 times, and evaluation metrics are recorded at each step, in addition to recording the actual performance of what is predicted to be the
best candidate within the population.
Each simulation is given a time budget equal to $min(E^{s})ND_{\mu}$, where $N$ is the size of the NASBench 201 search space, and $D$ represents NASBench 201 training durations.
This is intended to allow the possiblity of only the smallest prediction epoch scalar exploring the entire NASBench201 search space within the time budget.

Individual experiment results are available in the Aging Tournament Search section in the appendix.

\begin{figure}[!t]
    \begin{center}
        \resizebox{\columnwidth}{!}{
            \input{../res/figures/64_evosim_all_performances_over_size.pgf}
        }
        \caption{NASBench 201 Evolution Simulation: Performances vs History Size.
        Simulations with different prediction epoch scalars are identified by different stopping points.
        A prediction epoch scalar of .125 generally outperforms .25, but a PES of 1 outperforms all others.}
        \label{fig:nasbench_best_perf_vs_size}
    \end{center}
\end{figure}

\begin{figure}[!ht]
    \begin{center}
        \resizebox{\columnwidth}{!}{
            \input{../res/figures/64_evosim_all_performances_over_slices.pgf}
        }
        \caption{NASBench 201 Evolution Simulation: Time Utilized vs Best Configuration. 
        PES 1 and WS 1 outperforms other configurations for approximately 75\% of the time budget, 
        and configurations with PES 1 and any window scalar outperform other configurations for approximately 90\% of the time budget.}
        \label{fig:nasbench_best_perf_vs_slice}
    \end{center}
\end{figure}


\subsubsection{Analysis}

\textbf{Searches with early stopping only result in better models at low time budgets.}
As shown in Figure ~\ref{fig:nasbench_best_perf_vs_slice}, searches with early stopping only outperform searches without early stopping, coined \emph{normal searches},
early on within the allotted time budget. The degree of outperformance of all other searches by normal search is showcased in ~\ref{fig:nasbench_best_perf_vs_size}, 
which shows the actual performance of the candidate predicted to be the best within the population for each grid search configuration,
in respect to the number of models evalued so far.

These results provide an answer to the part of the focus of this experiment: Using a statistical approach for early stopping and thus search time reduction
is shown to only provide advantages with low time budgets.
Specifically, this is case for this experiment when the budget is less than 2.5\% of the expected time to fully explore the NASBench 201 search space.

\textbf{Normal search without early stopping performs better with windows that contain more than the last epoch.}
Within these simulations, normal search performs best with a window over all epochs, rather than over only the last epoch which represents the true final candidate performance.
This suggests that candidates that perform better overall tend to be close to the candidates with the best performances within the search space.
This is impactful, since this implies that the traditional method of using final performace to rank a candidate might not be the optimal
metric for determining the best candidate during an evolutionary selection step.
This comes with the stipulation that the time budget only allows partial exploration of the search space without early stopping.
To examine the impact of a larger time budget on the performance of different windows with normal search, an ablation study will be conducted.

\textbf{RM outperforms ZM in most cases.}
In the vast majority of cases, RM results in lower PARE and PNRE measurements, and higher Spearman Coefficients.
This is represented in Table ~\ref{table:full_sim_nasnet_convergence}, where all measurement ratios with 5\% or more advantage are to the advantage of RM.

\subsection{Ablation Study: The Impact of Windows on Normal Searches}

This study will be composed of a narrowed version of the previous evolutionary experiment, where normal search will evaluated with a larger time budget.
Specifically, the search budget will be equal to the expected duration to fully explore the NASBench 201 search space with a brute-force search.
It should be noted that the aging tournament selection algorithm encompases an identity mechanism which allows already-evaluated candidates to exist within the population,
therefore not guaranteeing that only new architectures are added to the population's history.
As a result of this, the algorithm does not provide the guarantee that the entire search space will be explored within the expected duration for full exploration.
This caveat is reconciled by the fact that NASBench 201 contains a narrow search space in comparison to search spaces such as the NASNet variant used in this work,
meaning applications of this method outside of NASBench 201 are far less likely to have the possibility of exploring the entire search space and consequently suffering from using
a search method besides brute-force search.
Individual results for this experiement can be found in the Aging Tournament Ablation Study subsection in the appendix.

\begin{figure}[!h]
    \begin{center}
        \resizebox{\columnwidth}{!}{
            \input{../res/figures/64_extended_evosim_all_performances_over_size.pgf}
        }
        \caption{NASBench 201 Extended Evolution Simulation: Performances vs History Size}
        \label{fig:evo_ext_nasbench_all_perfs}
    \end{center}
\end{figure}

\subsubsection{Analysis}

Besides the very initial time steps in each search, searches with a window of 1 are shown to result in higher performances at earlier points in comparsion to all other
window sizes (Figure~\ref{fig:evo_ext_nasbench_best_at_slice} in the appendix).
Figure~\ref{fig:evo_ext_nasbench_all_perfs} shows that searches using only the last epoch's performance appear to be slowly converging toward the same performances, 
but further expansion of the search time budget in additional experiments loses meaninfulness since the search space would at that point be better explored via brute force.
The impact of windows on normal search will thus need further evaluation in larger search spaces and with larger time budgets in a future study.
Regardless, these results suggest that search times are at least initially improved by the use of a window over the entirety of trained epochs.

\section{Conclusion}

In this work, two proxy performance metrics are measured over varying averaged windows, at varying early stopping points to predict relative final candidate performances within a population.
This method is evaluated within the contexts of stochastic search and aging tournament search to determine the efficacy of the method when applied to neural architecture search.
The primary contribution of this work is the demonstration that the use of a windowed proxy performance metric within evolutionary architecture search,
results in higher performances at fixed time budgets in comparison to direct final performance measurement, within the NASBench 201 search space.
If, in future work, this method is shown to carry over to different search spaces and macro-architectures, this provides a
means to further improve search speeds and search results that is highly applicable to neural architecture search methods that do not already utilize a performance surrogate nor early stopping.
Additionally, this work demonstrates that performance prediction with this method in conjunction with early stopping does not result in increased search efficiency.

\section{Future Work}

As discussed in the ablation study, future directions include the verification of the primary contribution within larger search spaces and with larger search times or compute budgets.
Additionally, the investigation into factors influencing the normality of population performance distributions across different search spaces
could yield opportunities for approaches that rely on the normality of population distributions. 
This is contingent on the absence of the possiblity for vanishing gradients among other similar problems, which otherwise would
lead to inevitable clusters within the population.

\FloatBarrier

\bibliography{reference}

\FloatBarrier
\appendix
\section{Appendix}

\subsection{Stochastic Search Results}
\clearpage
\subsubsection{NASNet Simulations}

\begin{figure}[!h]
    \begin{center}
        \resizebox{0.9\columnwidth}{!}{
            \input{../res/figures/nasnet_1x_acceleration.pgf} 
        }
        \caption{NASNet 16p: 1 Prediction Epoch Scalar}
        \label{fig:nasnet_1x_acceleration}
    \end{center}
\end{figure}

\begin{figure}[!h]
    \begin{center}
        \resizebox{0.95\columnwidth}{!}{
            \input{../res/figures/nasnet_2x_acceleration.pgf} 
        }
        \caption{NASNet 16p: 0.5 Prediction Epoch Scalar}
        \label{fig:nasnet_2x_acceleration}
    \end{center}
\end{figure}
        
\begin{figure}[!h]
    \begin{center}
        \resizebox{0.95\columnwidth}{!}{
            \input{../res/figures/nasnet_4x_acceleration.pgf}
        }
        \caption{NASNet 16p: 0.25 Prediction Epoch Scalar}
        \label{fig:nasnet_4x_acceleration}
    \end{center}
\end{figure}

\begin{figure}[!h]
    \begin{center}
        \resizebox{0.95\columnwidth}{!}{
            \input{../res/figures/nasnet_8x_acceleration.pgf}
        }
        \caption{NASNet 16p: 0.125 Prediction Epoch Scalar}
        \label{fig:nasnet_8x_acceleration}
    \end{center}
\end{figure}

\FloatBarrier
\clearpage
\subsubsection{NASBench201 Simulations}

\begin{figure}[!h]
    \begin{center}
        \resizebox{0.9\columnwidth}{!}{
            \input{../res/figures/nasbench_16_1x_acceleration.pgf}
        }
        \caption{NASBench 201 16p: 1 Prediction Epoch Scalar}
        \label{fig:nasbench_16_1x_acceleration}
    \end{center}
\end{figure}

\begin{figure}[!h]
    \begin{center}
        \resizebox{0.95\columnwidth}{!}{
            \input{../res/figures/nasbench_16_2x_acceleration.pgf}
        }
        \caption{NASBench 201 16p: 0.5 Prediction Epoch Scalar}
        \label{fig:nasbench_16_2x_acceleration}
    \end{center}
\end{figure}

\begin{figure}[!h]
    \begin{center}
        \resizebox{0.95\columnwidth}{!}{
            \input{../res/figures/nasbench_16_4x_acceleration.pgf}
        }
        \caption{NASBench 201 16p: 0.25 Prediction Epoch Scalar}
        \label{fig:nasbench_16_4x_acceleration}
    \end{center}
\end{figure}

\begin{figure}[!h]
    \begin{center}
        \resizebox{0.95\columnwidth}{!}{
            \input{../res/figures/nasbench_16_8x_acceleration.pgf}
        }
        \caption{NASBench 201 16p: 0.125 Prediction Epoch Scalar}
        \label{fig:nasbench_16_8x_acceleration}
    \end{center}
\end{figure}

\begin{figure}[!h]
    \begin{center}
        \resizebox{0.9\columnwidth}{!}{
            \input{../res/figures/nasbench_100_1x_acceleration.pgf}
        }
        \caption{NASBench 201 100p: 1 Prediction Epoch Scalar}
        \label{fig:nasbench_100_1x_acceleration}
    \end{center}
\end{figure}

\begin{figure}[!h]
    \begin{center}
        \resizebox{0.95\columnwidth}{!}{
            \input{../res/figures/nasbench_100_2x_acceleration.pgf}
        }
        \caption{NASBench 201 100p: 0.5 Prediction Epoch Scalar}
        \label{fig:nasbench_100_2x_acceleration}
    \end{center}
\end{figure}

\begin{figure}[!h]
    \begin{center}
        \resizebox{0.95\columnwidth}{!}{
            \input{../res/figures/nasbench_100_4x_acceleration.pgf}
        }
        \caption{NASBench 201 100p: 0.25 Prediction Epoch Scalar}
        \label{fig:nasbench_100_4x_acceleration}
    \end{center}
\end{figure}

\begin{figure}[!h]
    \begin{center}
        \resizebox{0.95\columnwidth}{!}{
            \input{../res/figures/nasbench_100_8x_acceleration.pgf}
        }
        \caption{NASBench 201 100p: 0.125 Prediction Epoch Scalar}
        \label{fig:nasbench_100_8x_acceleration}
    \end{center}
\end{figure}


\clearpage
\FloatBarrier
\onecolumn
\subsubsection{Stochastic Simulation Points of Convergence}

\begin{table}[!h]
    \begin{center}
        \resizebox{\textwidth}{!}{
            \pgfplotstabletypeset[
                    col sep=comma,
                    trim cells=true,
                    every head row/.style={before row=\toprule,after row=\midrule},
                    every last row/.style={after row=\bottomrule},
                    every column/.style={numeric type,fixed,precision=3,dec sep align}
                    ]{../res/figures/nasnet_convergences.csv}
        }
        \caption{NASNet 16p: Points of Convergence}
        \label{table:nasbench_16_convergence}
    \end{center}
\end{table}
\begin{table}[!h]
    \begin{center}
        
        \resizebox{\textwidth}{!}{
            \pgfplotstabletypeset[
                col sep=comma,
                trim cells=true,
                every head row/.style={before row=\toprule,after row=\midrule},
                every last row/.style={after row=\bottomrule},
                every column/.style={numeric type,fixed,precision=3,dec sep align}
                ]{../res/figures/nasbench_16_convergences.csv}
        }
        \caption{NASBench 201 16p: Points of Convergence}
        \label{table:nasnet_convergence}
    \end{center}
\end{table}
\begin{table}[!h]
    \begin{center}
        
        \resizebox{\textwidth}{!}{
            \pgfplotstabletypeset[
                col sep=comma,
                trim cells=true,
                every head row/.style={before row=\toprule,after row=\midrule},
                every last row/.style={after row=\bottomrule},
                every column/.style={numeric type,fixed,precision=3,dec sep align}
                ]{../res/figures/nasbench_100_convergences.csv}
        }
        \caption{NASBench 201 100p: Points of Convergence}
        \label{table:nasbench_100_convergence}
    \end{center}
\end{table}

\FloatBarrier
\subsection{Aging Tournament Search}
\FloatBarrier

\begin{table}[!h]
    \begin{center}
        \resizebox{\textwidth}{!}{
            \pgfplotstabletypeset[
                col sep=comma,
                trim cells=true,
                every head row/.style={before row=\toprule,after row=\midrule},
                every last row/.style={after row=\bottomrule},
                every column/.style={numeric type,fixed,precision=3,dec sep align}
                ]{../res/figures/64_full_sim_convergences.csv}
        }
        \caption{NASBench 201: Points of Convergence}
        \label{table:full_sim_nasnet_convergence}
    \end{center}
\end{table}
\twocolumn


\FloatBarrier
\clearpage



\begin{figure}
    \begin{center}
        \resizebox{\columnwidth}{!}{
            \input{../res/figures/64_evosim_1x_acceleration.pgf}
        }
        \caption{NASBench 201: 1 Prediction Epoch Scalar}
        \label{fig:evo_nasbench_1x_acceleration}
    \end{center}
\end{figure}

\begin{figure}
    \begin{center}
        \resizebox{\columnwidth}{!}{
            \input{../res/figures/64_evosim_2x_acceleration.pgf}
        }
        \caption{NASBench 201: 0.5 Prediction Epoch Scalar}
        \label{fig:evo_nasbench_2x_acceleration}
    \end{center}
\end{figure}

\begin{figure}
    \begin{center}
        \resizebox{\columnwidth}{!}{
            \input{../res/figures/64_evosim_4x_acceleration.pgf}
        }
        \caption{NASBench 201: 0.25 Prediction Epoch Scalar}
        \label{fig:evo_nasbench_4x_acceleration}
    \end{center}
\end{figure}

\begin{figure}
    \begin{center}
        \resizebox{\columnwidth}{!}{
            \input{../res/figures/64_evosim_8x_acceleration.pgf}
        }
        \caption{NASBench 201: 0.125 Prediction Epoch Scalar}
        \label{fig:evo_nasbench_8x_acceleration}
    \end{center}
\end{figure}

\FloatBarrier
\subsubsection{Aging Tournament Ablation Study}

\begin{figure}[!h]
    \begin{center}
        \resizebox{0.9\columnwidth}{!}{
            \input{../res/figures/64_extended_evosim_all_performances_over_slices.pgf}
        }
        \caption{NASBench 201: Time Utilized vs Best Configuration}
        \label{fig:evo_ext_nasbench_best_at_slice}
    \end{center}
\end{figure}

\begin{figure}[!h]
    \begin{center}
        \resizebox{0.9\columnwidth}{!}{
            \input{../res/figures/64_extended_evosim_1x_acceleration.pgf}
        }
        \caption{NASBench 201: 1 Prediction Epoch Scalar}
        \label{fig:evo_ext_nasbench_1x_acceleration}
    \end{center}
\end{figure}
\onecolumn

\subsection{NASNet 16p Population Details}

The NASNet 16p search space is similar to that of NASNet\cite{nasnet}, with only the modification of possible operations. 
Possible operations for this work's search space include the following:
\begin{multicols}{2}
\begin{itemize}
    \item Identity
    \item Convolution 3x3
    \item Depthwise Seperable Convolution 3x3
    \item Depthwise Seperable Convolution 5x5
    \item Depthwise Seperable Convolution 7x7
    \item Average Pooling 3x3
    \item Average Pooling 5x5
    \item Max Pooling 3x3
    \item Max Pooling 5x5
    \item Depthwise Seperable Convolution 1x7 followed by Depthwise Seperable Convolution 7x1
\end{itemize}
\end{multicols}

This results in the search space of $(10^{5}\cdot(6! - 2!))^{2} = 5.155\cdot10^{15}$ architectures, in comparison to the NASNet\cite{nasnet} space which
has $(13^{5}\cdot(6! - 2!))^{2} = 7.107\cdot10^{16}$ architectures.
The NASNet 16p population is comprised of the following six subpopulations specified by their hyperparameters, all of which are trained on Cifar-10.
The population is intended to represent an architecture near the base architecture used to evaluate Cifar-10 in NASNet\cite{nasnet}, in addition to a variety of smaller
proxies. Population sizes are limited to 16 models due to compute constraints.

\begin{table}[!h]
    \begin{center}
        \resizebox{0.9\columnwidth}{!}{
        \begin{tabular}{c | c | c | c | c | c}
            \toprule
            Subpopulation   & Layers & Normal Cells per Layer   & Filters   & Epochs   & Parameters ($\mu$)\\
            \midrule
            1               & 3      & 3                        & 24        & 16       & 1766642.5 \\
            2               & 3      & 3                        & 24        & 32       & 1766642.5 \\
            3               & 3      & 5                        & 24        & 16       & 2719517.5 \\
            4               & 3      & 5                        & 32        & 16       & 4735204 \\
            4               & 3      & 6                        & 32        & 16       & 5564662 \\
            4               & 3      & 6                        & 32        & 32       & 5564662 \\
            \bottomrule
        \end{tabular}
        }
        \caption{NASNet 16p Subpopulation Configurations}
        \label{table:nasnet_details}
    \end{center}
\end{table}

\subsection{Code}

Code for experiments in this work is available at \url{https://github.com/dkoleber/mpp_4_nastr}

\end{document}
